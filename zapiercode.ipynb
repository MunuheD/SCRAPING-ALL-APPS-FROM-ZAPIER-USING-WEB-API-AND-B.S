{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d36992-9e18-4fd8-a443-814c7ebc19e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time  # Import the time module for sleep\n",
    "\n",
    "# Define the API URL OF THE WEBSITE (AS OBSERVED IN CHROME'S DEVELOPMENT TOOL)\n",
    "url = \"https://zapier.com/explore-api\"\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"x-api-key\": \"YnhcscfPMog8CmM-28oHy_AyCEMJP!r36m3ZqDMf\",\n",
    "    \"x-csrftoken\": \"nQATqivRqac5Fpk5aW5W4lOLxITQ4YsdzBFreMy8KsDoIB3rmcSVgPtJVpBUS1Dl\"\n",
    "}\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "# DataFrame is preferred for easy comparative checking (easy to track errors)\n",
    "all_apps = pd.DataFrame()\n",
    "\n",
    "# Loop through offset values from 0 to 7712, with a step of 22\n",
    "for offset in range(0, 7713, 22):\n",
    "    # Define the payload with dynamic offset and constant limit\n",
    "    # I extracted the payload parameters from Network tab in Chrome's developer tool\n",
    "    data = {\n",
    "        \"operationName\": \"CategoryAppsBFFQuery\",\n",
    "        \"query\": \"\"\"\n",
    "        query CategoryAppsBFFQuery($categorySlug: String = \"all\", $limit: Int = 10, $offset: Int = 0, $orderBy: AppSortOrder, $filterBy: String) {\n",
    "          appCategory: appCategoryWithSlug(slug: $categorySlug) {\n",
    "            id\n",
    "            title\n",
    "            apps(\n",
    "              orderBy: $orderBy\n",
    "              limit: $limit\n",
    "              offset: $offset\n",
    "              additionalCategorySlug: $filterBy\n",
    "            ) {\n",
    "              results {\n",
    "                name\n",
    "                profileUrl\n",
    "                __typename\n",
    "              }\n",
    "              count\n",
    "              hasNextPage\n",
    "              __typename\n",
    "            }\n",
    "            __typename\n",
    "          }\n",
    "        }\n",
    "        \"\"\",\n",
    "        \"variables\": {\n",
    "            \"categorySlug\": \"all\",\n",
    "            \"limit\": 22,\n",
    "            \"offset\": offset,\n",
    "            \"orderBy\": \"POPULARITY\",\n",
    "            \"filterBy\": \"\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Send the POST request\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    \n",
    "    # Check the response status\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Request successful for offset {offset}\")\n",
    "        # Process the response JSON data\n",
    "        results = response.json()\n",
    "        \n",
    "        # Extract the relevant data from the response\n",
    "        apps = results['data']['appCategory']['apps']['results']\n",
    "        \n",
    "        # Append to the DataFrame (app name and profile URL)\n",
    "        df = pd.DataFrame(apps)[['name', 'profileUrl']]\n",
    "        all_apps = pd.concat([all_apps, df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code} for offset {offset}\")\n",
    "    \n",
    "    # Sleep for 2 seconds to rate limit requests (Ethical Scraping)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Display the entire DataFrame\n",
    "print(all_apps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b256ed2-fdbf-4a95-8bc0-0f91f3e4f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LETS NOW USE BEAUTIFUL SOUP TO SCRAPE THE TARGET CONTENT IN profileurls GENERATED ABOVE\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# THESE ARE THE profileUrl that we got in our previous code above\n",
    "profile_url_list = all_apps['profileUrl'].tolist()\n",
    "\n",
    "# LISTS TO STORE THE EXTRACTED DATA FROM EACH PAGE\n",
    "app_description = []\n",
    "app_url = []\n",
    "failed_urls = []  # To keep track of failed URLs\n",
    "\n",
    "# BATCH SIZE\n",
    "batch_size = 100\n",
    "\n",
    "# GET LIST OF BEAUTIFULSOUP OBJECTS\n",
    "def getpage(url_list):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    all_bs = []  # List to hold all BeautifulSoup objects\n",
    "    for url in url_list:  # Use the batch-specific URL list\n",
    "        attempts = 0\n",
    "        success = False  # Flag to track successful requests\n",
    "\n",
    "        while attempts < 10:  # Retry up to 10 times\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, timeout=10)  # Set a timeout\n",
    "                response.raise_for_status()  # Raise an error for bad responses (4xx or 5xx)\n",
    "                bs = BeautifulSoup(response.text, 'html.parser')\n",
    "                all_bs.append(bs)\n",
    "                success = True  # Mark success\n",
    "                time.sleep(2) # Ethical scraping, no exhausting the server\n",
    "                break  # Break after a successful request\n",
    "            except (requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:\n",
    "                attempts += 1\n",
    "                wait_time = min(60, 2 ** attempts)\n",
    "                print(f\"Connection error occurred: {e}. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                attempts += 1  # Increment attempts for any failed request\n",
    "                print(f\"Request failed: {e}. Retrying in 1 second...\")\n",
    "                time.sleep(1)\n",
    "\n",
    "        if not success:  # If the request was not successful after all attempts\n",
    "            print(f\"Failed to scrape {url} after {attempts} attempts.\")\n",
    "            failed_urls.append(url)\n",
    "            all_bs.append(None)\n",
    "\n",
    "    return all_bs\n",
    "\n",
    "# EXTRACT CONTENT\n",
    "def getcontent(bs_list):\n",
    "    for bs in bs_list:\n",
    "        if bs is None:\n",
    "            app_description.append(\"not given\")\n",
    "            app_url.append(\"not given\")\n",
    "            continue\n",
    "\n",
    "        # Extracting app description\n",
    "        app_details = bs.find('div', {'class': 'css-8gold7-AppDetails__appDescription'})\n",
    "        if app_details:\n",
    "            app_description.append(app_details.get_text())\n",
    "        else:\n",
    "            app_description.append(\"not given\")\n",
    "\n",
    "        # Extracting URLs for \"Learn more\"\n",
    "        found_url = False\n",
    "\n",
    "        for link in bs.find_all('a', href=True):\n",
    "            if link.find(string=\"Learn more\"):\n",
    "                href_value = link['href']\n",
    "                app_url.append(href_value)\n",
    "                found_url = True\n",
    "\n",
    "        if not found_url:\n",
    "            app_url.append(\"not given\")\n",
    "\n",
    "# PROCESS IN BATCHES (This approach helps to save the RAM and track record easily)\n",
    "def process_batches(profile_url_list, batch_size):\n",
    "    app_names = all_apps['name'].tolist()\n",
    "    \n",
    "    # Start from o to 7717 (len(profile_url_list)), in steps of 100 (batch size)\n",
    "    for start_idx in range(0, len(profile_url_list), batch_size):\n",
    "        end_idx = start_idx + batch_size\n",
    "        profile_url_batch = profile_url_list[start_idx:end_idx]\n",
    "        app_name_batch = app_names[start_idx:end_idx]\n",
    "        \n",
    "        print(f\"Processing batch from {start_idx} to {end_idx}\")\n",
    "        \n",
    "        # Get BeautifulSoup objects for the batch\n",
    "        bs_list = getpage(profile_url_batch)\n",
    "        \n",
    "        # Extract content from the batch\n",
    "        getcontent(bs_list)\n",
    "        \n",
    "        # ARRANGING THE DATA FOR BETTER PRINTING\n",
    "        if len(app_description) == len(app_name_batch) and len(app_url) == len(app_name_batch):\n",
    "            ZapierScrapingResults = pd.DataFrame({\n",
    "                \"App Name\": app_name_batch,\n",
    "                \"Description\": app_description,\n",
    "                \"URL\": app_url\n",
    "            })\n",
    "            # Save to each batch to Excel\n",
    "            ZapierScrapingResults.to_excel(rf\"E:\\PYTHON- DATA SCIENCE\\Data hub\\ZapierScrapingResults_batch_{start_idx//batch_size + 1}.xlsx\", index=False)\n",
    "            print(f\"Batch {start_idx//batch_size + 1} saved successfully.\")\n",
    "        else:\n",
    "            print(\"Length of lists does not match to create the DataFrame for this batch.\")\n",
    "        \n",
    "        # Clear lists after saving the batch\n",
    "        app_description.clear()\n",
    "        app_url.clear()\n",
    "        print(f\"Memory cleared after saving batch {start_idx//batch_size + 1}.\")\n",
    "\n",
    "# RUN THE BATCH PROCESSING\n",
    "process_batches(profile_url_list, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fcc78a-3fba-428a-a91a-55d6c955975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FINAL STEP IS TO COMBINE ALL EXCEL FILES GENERATED ABOVE INTO A SINGLE EXCEL FILE\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Path where batch files downloaded above are saved\n",
    "path = r\"E:\\PYTHON- DATA SCIENCE\\Data hub\"  # Replace this with your own storage path\n",
    "\n",
    "# Create a list of all files matching your file name pattern\n",
    "file_pattern = os.path.join(path, \"ZapierScrapingResults_batch_*.xlsx\")\n",
    "all_files = glob.glob(file_pattern)\n",
    "\n",
    "# Read and concatenate all files\n",
    "df_list = [pd.read_excel(file) for file in all_files]  # Read each file into a DataFrame\n",
    "combined_df = pd.concat(df_list, ignore_index=True)  # Concatenate all DataFrames\n",
    "\n",
    "# Save the combined DataFrame to a new Excel file\n",
    "combined_df.to_excel(os.path.join(path, \"Combined_ZapierScrapingResults.xlsx\"), index=False)\n",
    "\n",
    "print(r\"All files have been successfully concatenated and saved in E:\\PYTHON- DATA SCIENCE\\Data hub.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
